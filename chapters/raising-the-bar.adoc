== TEST: Raising the Bar - The Pursuit of Quality in Agile Projects

We want to ensure that what we deliver to our customers is something we can be proud of, it's not just good - it's awesome! We therefore need to ensure that each-and-every Story is cross-referenced and checked with 1 or more test cases.

Additionally, if we want to ensure we are delivering high-quality high-value (HQHV) we need to ensure we are performing testing earlier (a ‚ÄúShift Left‚Äù of testing) rather than leaving it to the last step before deployment. For this reason, the Test Pilot is an integral member of the team who will be writing test cases when the first line of code is being written.

=== The importance of acceptance criteria

In order to ensure that each and every Story can be validated, a User Story should have an acceptance criteria to make it super clear to the developer what the business are accepting, and how the Test Pilot will be testing your solution. Let‚Äôs get the ‚Äústory‚Äù straight before we start with delivering value as to what everybody‚Äôs expectations are. An example acceptance criteria for bug resolution could be:

*	100% of major and critical bugs will be resolved
*	50% of minor bugs will be resolved
*	Cosmetic issues will be fixed on a best endeavour basis

=== Test Plans, Suites & Cases

A test plan is a group of test cases. We recommend a Test Plan is made up from a number of Test Suites (one for each Feature). You then create a test case per user story.

A test case is the steps that validate a user story. Each step should have an expected result. A tester can thus validate the test case step by step, and create a bug at the designated step that caused the failure. Along with a screenshot (always include a screenshot and any other relevant information), this should give developers the resources necessary to troubleshoot the majority of issues.

=== Feautures vs Bugs vs Enhancements

Knowing the difference between a bug, enhancement, and feature is the key to smarter decisions and smoother teamwork.

A Bug is any defect that causes a User Story to fail its acceptance criteria. These are the gaps ‚Äî the things that were supposed to work but don‚Äôt. Bugs are always tied to specific requirements and need immediate attention because they compromise the agreed definition of ‚Äúdone.‚Äù They are prioritised by impact - Critical, Major, Minor, or Cosmetic - which directly influences sprint planning and triage.

An Enhancement is an improvement request that does not block the acceptance of a User Story. Think usability tweaks, minor performance improvements, or visual polish ‚Äî the stuff that makes good software feel great. If implementing an enhancement takes more than 4 hours, it should be treated as a Feature instead - scoped and planned as such.

A Feature is a new capability ‚Äî a set of User Stories that introduces fresh functionality or a significant change to existing behavior. Features are the strategic layer: they expand the product‚Äôs scope and value. They are planned during sprint planning and oftenrequire coordinated sedign, development, and QA. Any request that breaks new ground - technically or funcionally - is a feature.

This classification ensures we‚Äôre fixing what‚Äôs broken, refining what‚Äôs working, and delivering what‚Äôs next ‚Äî all without losing sight of why we're building in the first place.

=== Static code analysis

Static code analysis is where the code of a solution is analysed without it having to be build and executed. It ensures that the code adheres to industry and team standards. Performing static code analysis can:

*	Improve the ability for developers to understand each other‚Äôs code
*	Reduce the time needed for a peer review
*	Catch issues that may not otherwise be caught

=== Automated UI testing

To speed up the repeatable nature of testing to help improve the cadence of release, where possible we should leverage the use of automated test tools such as Selenium Web Driver .

=== Security assessment & compliance

Consider security during the security and compliance using automated tools such as Microsoft Security Code Analysis , Secure DevOps Kit for Azure  and SME reviews. Best to find out early with regards to security issues prior to hitting production!

Ensure that you are utilizing all the applicable security options available to you in the cloud hosting environment. For example, ensure you have policies set, firewalls configured, threat protection, encryption, monitoring, auditing and security event management.

You can utilize more than a single security step in your release pipeline to ensure better coverage.

=== Performance testing

Performance testing your solution allows you to confirm that your deployment will handle the loads in production. It also allows you to see where in the stack performance improvements can be made.

There are a number of tools available to carry out performance testing such as LoadRunner and Apache JMeter. JMeter is a popular free open source tool that supports many different protocols.

Integrating automated performance tests into your release process means we are thinking about the performance of the solution up front, and not just an after thought when users start complaining there are performance issues.

Typically you would carry out performance testing in the Test (QA) environment. To get results that are reflect of the Operations environment, the Test environment will need the same configuration in terms of sizing as Operations. Fortunately, if costs need to be kept to a minimum, the Test environment can be scaled up and down for doing performance testing.

=== User acceptance testing

User acceptance testing is where the client tests your solution to confirm the delivery meets their needs.

This is where our Test Plan acceptance criteria comes into play. No software is completely bug free so set expectations. If you have structured your post-launch support arrangement correctly they will be happy that any small work items can still get resolved under the support arrangement. They‚Äôre not just going to be stuck with any remaining issues!

Because your stakeholders most likely won‚Äôt have time to go through all test cases written for your system (well, they‚Äôll tell you that anyway üòâ), for use acceptance testing, create a smaller Test Plan that cover a broad range and the important stories that are representative of the overall Feature working well. NB The test cases should be written in a simple way anyway that any team member or stakeholder should be able to follow them.

=== Test Script Best Practices: Your Pre-Commit Safety Net

Before we dive into the advanced testing strategies, let's establish a foundation of solid test script practices that every engineer should follow. These aren't just guidelines for Test Pilots ‚Äì they're essential practices that help engineers catch issues early and maintain code quality throughout the development process.

Think of test scripts as your first line of defense against bugs. When engineers run these scripts before committing code, they create a safety net that catches issues before they ever reach the testing phase. This "shift left" approach to quality means problems are identified and resolved when they're cheapest and easiest to fix ‚Äì right at the developer's desk.

==== Keep Scripts Simple and Focused

The best test scripts are elegantly simple. Each script should have one clear purpose: to verify that specific functionality works as expected. Avoid the temptation to create elaborate, all-encompassing scripts that try to test everything at once. Instead, focus on creating focused scripts that confirm core functionality without unnecessary complexity.

Simple scripts are easier to maintain, faster to run, and more reliable. When a script fails, you want to immediately know what broke ‚Äì not spend time debugging the test itself. Think of it like a doctor's stethoscope: it's a simple tool that provides clear, immediate feedback about what's happening.

==== Use Condition-Based Waits, Not Arbitrary Delays

One of the most common mistakes in test scripts is using arbitrary time delays (like "wait 5 seconds") instead of condition-based waits. These sleeps make scripts slower and less reliable ‚Äì sometimes 5 seconds isn't enough, sometimes it's far too much.

Instead, use condition-based waits that check for specific states: "wait until the element is visible," "wait until the API returns a response," or "wait until the file exists." This makes your scripts both faster and more reliable, adapting to the actual system performance rather than arbitrary timeouts.

==== Capture Screenshots for Debugging Evidence

Screenshots are your debugging best friend. Configure your test scripts to automatically capture screenshots at key points, especially when something goes wrong. These visual breadcrumbs help you (and your teammates) quickly understand what happened during script execution.

Set up your scripts to save screenshots in a dedicated `/screenshots/` subdirectory, and make sure this directory is excluded from version control using `.gitignore`. Screenshots are valuable for debugging but don't belong in your repository ‚Äì they're generated artifacts, not source code.

==== Enable Comprehensive Console Logging

Enable verbose console logging in your test scripts. Log what the script is doing, what it's checking, and what it finds. When something goes wrong, these logs become invaluable for troubleshooting. Structure your logs to be clear and searchable:

```
[INFO] Starting user registration test
[DEBUG] Navigating to /signup page
[DEBUG] Filling form with test data
[INFO] Submitting registration form
[ERROR] Registration failed: Email already exists
[DEBUG] Screenshot saved: /screenshots/registration_error_20240315_143022.png
```

Good logging transforms a mysterious failure into a clear story of what went wrong and where.

==== Organize Scripts in a Centralized Location

Keep all your test scripts in a single, well-organized folder structure. This makes them easy to find, maintain, and run. A typical structure might look like:

```
/tests/
  /unit/
  /integration/
  /e2e/
  /scripts/
    run_all_tests.sh
    setup_test_environment.sh
    cleanup_test_data.sh
  /screenshots/  (in .gitignore)
```

This organization makes it clear where to find tests, how to run them, and how they relate to each other.

==== Create a Master Script for Easy Execution

Develop a master script that runs all your test scripts in the correct order. This script should handle environment setup, execute all relevant tests, and provide a clear summary of results. Engineers should be able to run a single command like `./run_all_tests.sh` and get comprehensive feedback about the health of their changes.

Your master script should:
- Set up the test environment
- Run tests in logical order (unit first, then integration, then end-to-end)
- Capture and report results clearly
- Clean up after itself
- Exit with appropriate status codes for CI/CD integration

==== Minimize Script Maintenance Overhead

Avoid creating an excessive number of scripts that become a maintenance burden. Each script you create is code that needs to be maintained, updated, and debugged. Focus on creating scripts that provide maximum value with minimal maintenance overhead.

Before creating a new script, ask: "Does this provide unique value that isn't covered by existing scripts?" If you can achieve the same coverage by enhancing an existing script, that's often the better choice.

==== Run Scripts Before Every Commit

This is perhaps the most important practice: engineers should run the full test suite before making any commit, and certainly before pushing code. This creates a culture where broken code rarely makes it into the main branch, reducing the debugging burden on Test Pilots and the entire team.

Think of it as professional discipline ‚Äì like a pilot running through pre-flight checks before takeoff. These scripts are your pre-commit checklist, ensuring that your code is ready for the next phase of the development process.

By following these practices, you create a robust foundation for quality that supports the entire development lifecycle. When engineers consistently use well-designed test scripts, they catch issues early, maintain code quality, and create a smoother experience for everyone downstream in the process.

===	A/B testing

A/B testing allows you to setup two deployment slots (A and B) in production  where one of those slots is used to deploy a new release to. So you have a hot slot and a staging slot. Using traffic routing, you can then send a percentage of the traffic to the new slot. That way, without affecting all users, you can have a period of testing in production where any new introduced issues should not affect all users.

A/B testing also allows high performance sites to always be available with zero warm up time needed. You deploy to the staging slot, start the slot running, and once running, swap this slot to be the production slot.

You can also look at implementing an intelligent solution that will analyse the logs and dynamically amend the routing rules to pass users to the new code on confirmation that there are no detrimental issues affecting performance or correct behaviour. Or, alternatively, fall back to the old code if an issue is detected.
